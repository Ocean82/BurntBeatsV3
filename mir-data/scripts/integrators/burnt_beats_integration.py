
#!/usr/bin/env python3
"""
Main Integration Script for Burnt Beats MIR Pipeline
Connects MIR analysis with the existing Burnt Beats platform
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json
import logging
from datetime import datetime

# Import Burnt Beats MIR components
from mir-data.scripts.loaders.audio_loader import BurntBeatsMIRLoader
from mir-data.scripts.extractors.feature_extractor import BurntBeatsFeatureExtractor, BurntBeatsVoiceAnalyzer
from mir-data.scripts.analyzers.genre_classifier import BurntBeatsGenreAnalyzer

# Import existing Burnt Beats components
try:
    from music_service import app as music_service
    from backend_repl.main import BurntBeatsVoiceEngine
except ImportError:
    logging.warning("Some Burnt Beats components not available for import")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BurntBeatsMIRPipeline:
    """Main pipeline integrating MIR analysis with Burnt Beats platform"""
    
    def __init__(self, mir_data_path: str = "mir-data"):
        self.mir_path = Path(mir_data_path)
        
        # Initialize components
        self.loader = BurntBeatsMIRLoader(mir_data_path)
        self.feature_extractor = BurntBeatsFeatureExtractor()
        self.voice_analyzer = BurntBeatsVoiceAnalyzer()
        self.genre_analyzer = BurntBeatsGenreAnalyzer(mir_data_path)
        
        # Ensure directories exist
        self.setup_directories()
        
        logger.info("Burnt Beats MIR Pipeline initialized")
    
    def setup_directories(self):
        """Setup required directory structure"""
        required_dirs = [
            "audio/samples",
            "audio/stems", 
            "audio/generated",
            "annotations/beats",
            "annotations/chords",
            "annotations/melody",
            "annotations/tempo",
            "features/mfcc",
            "features/chroma",
            "features/spectral",
            "spectrograms/mel",
            "spectrograms/cqt",
            "spectrograms/piano-rolls",
            "models"
        ]
        
        for dir_path in required_dirs:
            (self.mir_path / dir_path).mkdir(parents=True, exist_ok=True)
    
    def process_generated_song(self, audio_path: str, song_metadata: Dict) -> Dict:
        """Process a song generated by Burnt Beats AI"""
        track_id = song_metadata.get('song_id', f"generated_{int(datetime.now().timestamp())}")
        
        logger.info(f"Processing generated song: {track_id}")
        
        # Load audio
        audio, sr = self.loader.load_audio(audio_path, sr=22050)
        
        # Extract comprehensive features
        features = self.feature_extractor.extract_all_features(audio)
        
        # Save features
        features_path = self.mir_path / "features"
        self.feature_extractor.save_features(features, features_path, track_id)
        
        # Genre analysis
        genre_analysis = self.genre_analyzer.analyze_generated_music(features, song_metadata)
        
        # Compile analysis results
        analysis_results = {
            'track_id': track_id,
            'metadata': song_metadata,
            'processing_date': datetime.now().isoformat(),
            'audio_info': {
                'duration': len(audio) / sr,
                'sample_rate': sr,
                'channels': 1,  # Assuming mono after loading
                'rms_energy': np.sqrt(np.mean(audio**2))
            },
            'genre_analysis': genre_analysis,
            'features_extracted': list(features.keys()),
            'quality_metrics': self._calculate_quality_metrics(features, audio, sr)
        }
        
        # Save analysis results
        self._save_analysis_results(analysis_results, track_id)
        
        return analysis_results
    
    def process_voice_sample(self, audio_path: str, voice_metadata: Dict) -> Dict:
        """Process a voice sample for RVC training compatibility"""
        voice_id = voice_metadata.get('voice_id', f"voice_{int(datetime.now().timestamp())}")
        
        logger.info(f"Processing voice sample: {voice_id}")
        
        # Load audio
        audio, sr = self.loader.load_audio(audio_path, sr=22050)
        
        # Voice-specific analysis
        voice_analysis = self.voice_analyzer.analyze_voice_sample(audio)
        
        # Extract voice features
        voice_features = self.voice_analyzer.extract_voice_features(audio)
        
        # Save voice features
        voice_features_path = self.mir_path / "features" / "voice"
        voice_features_path.mkdir(exist_ok=True)
        
        for feature_name, feature_data in voice_features.items():
            np.save(voice_features_path / f"{voice_id}_{feature_name}.npy", feature_data)
        
        # Compile voice analysis
        analysis_results = {
            'voice_id': voice_id,
            'metadata': voice_metadata,
            'processing_date': datetime.now().isoformat(),
            'voice_analysis': voice_analysis,
            'rvc_compatibility': voice_analysis.get('suitable_for_rvc', False),
            'recommendations': self._generate_voice_recommendations(voice_analysis)
        }
        
        # Save voice analysis
        self._save_voice_analysis(analysis_results, voice_id)
        
        return analysis_results
    
    def analyze_user_upload(self, audio_path: str, user_metadata: Dict = None) -> Dict:
        """Analyze user-uploaded audio for reference or training"""
        if user_metadata is None:
            user_metadata = {}
        
        track_id = user_metadata.get('track_id', f"upload_{int(datetime.now().timestamp())}")
        
        logger.info(f"Analyzing user upload: {track_id}")
        
        # Load audio
        audio, sr = self.loader.load_audio(audio_path)
        
        # Extract features
        features = self.feature_extractor.extract_all_features(audio)
        
        # Genre analysis
        genre_analysis = self.genre_analyzer.analyze_generated_music(features, user_metadata)
        
        # Check if suitable for training data
        training_suitability = self._assess_training_suitability(features, audio, sr)
        
        analysis_results = {
            'track_id': track_id,
            'metadata': user_metadata,
            'processing_date': datetime.now().isoformat(),
            'genre_analysis': genre_analysis,
            'training_suitability': training_suitability,
            'audio_quality': self._assess_audio_quality(features, audio, sr)
        }
        
        return analysis_results
    
    def _calculate_quality_metrics(self, features: Dict, audio: np.ndarray, sr: int) -> Dict:
        """Calculate quality metrics for generated audio"""
        metrics = {}
        
        # Basic audio quality
        metrics['dynamic_range'] = np.max(audio) - np.min(audio)
        metrics['rms_energy'] = np.sqrt(np.mean(audio**2))
        metrics['peak_amplitude'] = np.max(np.abs(audio))
        
        # Spectral quality
        if 'spectral' in features:
            spectral = features['spectral']
            if 'spectral_centroid' in spectral:
                metrics['spectral_brightness'] = np.mean(spectral['spectral_centroid'])
            if 'spectral_rolloff' in spectral:
                metrics['spectral_rolloff'] = np.mean(spectral['spectral_rolloff'])
        
        # Harmonic content
        if 'harmonic' in features:
            harmonic = features['harmonic']
            if 'harmonic_component' in harmonic and 'percussive_component' in harmonic:
                harmonic_energy = np.sum(harmonic['harmonic_component']**2)
                percussive_energy = np.sum(harmonic['percussive_component']**2)
                total_energy = harmonic_energy + percussive_energy
                if total_energy > 0:
                    metrics['harmonic_ratio'] = harmonic_energy / total_energy
        
        return metrics
    
    def _assess_training_suitability(self, features: Dict, audio: np.ndarray, sr: int) -> Dict:
        """Assess if audio is suitable for training AI models"""
        suitability = {
            'overall_score': 0.0,
            'criteria': {},
            'suitable': False
        }
        
        # Duration check
        duration = len(audio) / sr
        suitability['criteria']['duration_ok'] = duration >= 10.0  # At least 10 seconds
        
        # Quality check
        rms_energy = np.sqrt(np.mean(audio**2))
        suitability['criteria']['quality_ok'] = rms_energy > 0.01  # Not too quiet
        
        # Dynamic range check
        dynamic_range = np.max(audio) - np.min(audio)
        suitability['criteria']['dynamic_range_ok'] = dynamic_range > 0.1
        
        # Calculate overall score
        score = sum(suitability['criteria'].values()) / len(suitability['criteria'])
        suitability['overall_score'] = score
        suitability['suitable'] = score >= 0.8
        
        return suitability
    
    def _assess_audio_quality(self, features: Dict, audio: np.ndarray, sr: int) -> Dict:
        """Assess overall audio quality"""
        quality = {}
        
        # Signal-to-noise ratio estimate
        rms_energy = np.sqrt(np.mean(audio**2))
        noise_floor = np.percentile(np.abs(audio), 10)  # Estimate noise floor
        if noise_floor > 0:
            quality['snr_estimate'] = 20 * np.log10(rms_energy / noise_floor)
        else:
            quality['snr_estimate'] = 60  # High SNR if no detectable noise floor
        
        # Clipping detection
        clipping_threshold = 0.95
        clipped_samples = np.sum(np.abs(audio) > clipping_threshold)
        quality['clipping_ratio'] = clipped_samples / len(audio)
        
        # Overall quality score
        quality_score = 1.0
        if quality['snr_estimate'] < 20:
            quality_score *= 0.5  # Reduce for low SNR
        if quality['clipping_ratio'] > 0.01:
            quality_score *= 0.5  # Reduce for clipping
        
        quality['overall_score'] = quality_score
        
        return quality
    
    def _generate_voice_recommendations(self, voice_analysis: Dict) -> List[str]:
        """Generate recommendations for voice sample improvement"""
        recommendations = []
        
        if voice_analysis.get('duration', 0) < 10:
            recommendations.append("Record longer samples (10+ seconds) for better voice cloning")
        
        if voice_analysis.get('quality_score', 0) < 0.5:
            recommendations.append("Improve recording quality - reduce background noise")
        
        if voice_analysis.get('f0_std', 0) < 10:
            recommendations.append("Include more pitch variation in the recording")
        
        if not voice_analysis.get('suitable_for_rvc', False):
            recommendations.append("Sample may not be suitable for RVC training - check audio quality and content")
        
        return recommendations
    
    def _save_analysis_results(self, results: Dict, track_id: str):
        """Save analysis results to file"""
        output_path = self.mir_path / "analysis_results" / f"{track_id}_analysis.json"
        output_path.parent.mkdir(exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"Analysis results saved for {track_id}")
    
    def _save_voice_analysis(self, results: Dict, voice_id: str):
        """Save voice analysis results"""
        output_path = self.mir_path / "voice_analysis" / f"{voice_id}_analysis.json"
        output_path.parent.mkdir(exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"Voice analysis saved for {voice_id}")
    
    def get_track_recommendations(self, track_id: str) -> Dict:
        """Get AI-powered recommendations for improving a track"""
        analysis_file = self.mir_path / "analysis_results" / f"{track_id}_analysis.json"
        
        if not analysis_file.exists():
            return {"error": "Analysis not found for track"}
        
        with open(analysis_file, 'r') as f:
            analysis = json.load(f)
        
        recommendations = {
            'genre_suggestions': [],
            'quality_improvements': [],
            'creative_suggestions': []
        }
        
        # Genre-based recommendations
        genre_analysis = analysis.get('genre_analysis', {})
        primary_genre = genre_analysis.get('genre_analysis', {}).get('primary_genre')
        confidence = genre_analysis.get('genre_analysis', {}).get('confidence', 0)
        
        if confidence < 0.7:
            recommendations['creative_suggestions'].append("Experiment with stronger genre characteristics")
        
        # Quality recommendations
        quality_metrics = analysis.get('quality_metrics', {})
        if quality_metrics.get('rms_energy', 0) < 0.1:
            recommendations['quality_improvements'].append("Consider increasing overall volume/energy")
        
        return recommendations

# Main integration function for Burnt Beats
def integrate_mir_with_burnt_beats():
    """Main function to set up MIR integration with Burnt Beats"""
    logger.info("Initializing Burnt Beats MIR Integration...")
    
    # Initialize pipeline
    mir_pipeline = BurntBeatsMIRPipeline()
    
    # Test with existing audio if available
    storage_path = Path("storage")
    if storage_path.exists():
        # Look for existing audio files
        for audio_file in storage_path.rglob("*.mp3"):
            if audio_file.stat().st_size > 0:  # Non-empty file
                logger.info(f"Found audio file for testing: {audio_file}")
                
                # Create test metadata
                test_metadata = {
                    'song_id': audio_file.stem,
                    'source': 'existing_storage',
                    'file_path': str(audio_file)
                }
                
                try:
                    # Process the file
                    analysis = mir_pipeline.process_generated_song(str(audio_file), test_metadata)
                    logger.info(f"Successfully analyzed: {audio_file.stem}")
                    logger.info(f"Primary genre: {analysis.get('genre_analysis', {}).get('genre_analysis', {}).get('primary_genre', 'unknown')}")
                    
                    break  # Process only one file for testing
                except Exception as e:
                    logger.error(f"Error processing {audio_file}: {e}")
    
    logger.info("Burnt Beats MIR Integration ready")
    return mir_pipeline

if __name__ == "__main__":
    # Initialize the integration
    pipeline = integrate_mir_with_burnt_beats()
    print("Burnt Beats MIR Pipeline ready for use!")
